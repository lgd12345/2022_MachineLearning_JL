{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c8c882-97c3-4cf7-8c25-3fc66447cb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173c46f9-2a96-4c6f-83b6-18dc5bfddb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968799a6-ff6a-4e33-b7cb-72ec80a09436",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, x): # 수치미분 debug version # Ctrl + A 모두 선택\n",
    "    delta_x = 1e-4 \n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "#     print(\"debug 1. initial input variable =\", x)\n",
    "#     print(\"debug 2. initial input grad =\", grad)\n",
    "#     print(\"================================== =\")\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx=it.multi_index\n",
    "        \n",
    "#         print(\"debug 3, idx=\",idx,\",x[idx]=\",x[idx])\n",
    "        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx]=float(tmp_val)+delta_x\n",
    "        fx1=f(x) #f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx]=(fx1-fx2)/(2*delta_x)\n",
    "        \n",
    "#         print(\"debug 4. grad[idx] =\",grad[idx])\n",
    "#         print(\"debug 5. grad = \", grad)\n",
    "#         print(\"=================================\")\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it.iternext() \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee7ea886-a71e-44b4-8eac-c5a1f72f5927",
   "metadata": {},
   "outputs": [],
   "source": [
    "#딥러닝으로 게이트 만들기\n",
    "class LogicGate:\n",
    "    def __init__(self,gate_name,xdata,tdata):\n",
    "#         입력층 2개\n",
    "        self.name=gate_name\n",
    "        self.__xdata=xdata\n",
    "        self.__tdata=tdata\n",
    "        \n",
    "        self.__xdata=xdata.reshape(4,2)\n",
    "        self.__tdata=tdata.reshape(4,1)\n",
    "#         은닉층 한층에 노드를 6개\n",
    "        self.__w2=np.random.rand(2,6)\n",
    "        self.__b2=np.random.rand(1)\n",
    "#         출력층 한개\n",
    "        self.__w3=np.random.rand(6,1)\n",
    "        self.__b3=np.random.rand(1)\n",
    "        \n",
    "        self.__learning_rate=1e-2\n",
    "        \n",
    "# 입력층 은닉층 출력층\n",
    "        \n",
    "    def feed_forward(self):\n",
    "#         입력층\n",
    "        delta=1e-7\n",
    "#         은닉층\n",
    "        z2=np.dot(self.__xdata, self.__w2)+self.__b2\n",
    "        a2=sigmoid(z2)\n",
    "#         출력층\n",
    "        z3=np.dot(a2, self.__w3)+self.__b3\n",
    "        y=sigmoid(z3)\n",
    "        \n",
    "        return -np.sum(self.__tdata*np.log(y+delta)+(1-self.__tdata)*np.log((1-y)+delta))\n",
    "    \n",
    "#     트레이닝\n",
    "    def train(self):\n",
    "        f=lambda x:self.feed_forward()\n",
    "        print(\"Initial loss func: \",self.feed_forward())\n",
    "        \n",
    "        for step in range(10001):\n",
    "            self.__w2-=self.__learning_rate*numerical_derivative(f,self.__w2)\n",
    "            self.__b2-=self.__learning_rate*numerical_derivative(f,self.__b2)\n",
    "            \n",
    "            self.__w3-=self.__learning_rate*numerical_derivative(f,self.__w3)\n",
    "            self.__b3-=self.__learning_rate*numerical_derivative(f,self.__b3)\n",
    "            \n",
    "            if step %400==0:\n",
    "                print(\"step: \",step,\", loss_value: \",self.feed_forward())\n",
    "                \n",
    "    def predict(self, xdata):\n",
    "        z2=np.dot(xdata, self.__w2)+ self.__b2\n",
    "        a2=sigmoid(z2)\n",
    "        \n",
    "        z3=np.dot(a2, self.__w3)+ self.__b3\n",
    "        y=sigmoid(z3)\n",
    "        \n",
    "        if y>=0.5:\n",
    "            result=1\n",
    "            \n",
    "        else:\n",
    "            result=0\n",
    "            \n",
    "        return y,result        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd0d4d62-24ad-4957-b1ba-fe5fafaa2ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss func:  7.029834723249024\n",
      "step:  0 , loss_value:  6.7548146694021804\n",
      "step:  400 , loss_value:  2.2938924996753296\n",
      "step:  800 , loss_value:  2.2128284877434434\n",
      "step:  1200 , loss_value:  2.093426745375205\n",
      "step:  1600 , loss_value:  1.882107322858039\n",
      "step:  2000 , loss_value:  1.6441988520137802\n",
      "step:  2400 , loss_value:  1.402160461011797\n",
      "step:  2800 , loss_value:  1.1367985672547136\n",
      "step:  3200 , loss_value:  0.8784755078791389\n",
      "step:  3600 , loss_value:  0.6650037506336989\n",
      "step:  4000 , loss_value:  0.5062291139014178\n",
      "step:  4400 , loss_value:  0.39320429371766213\n",
      "step:  4800 , loss_value:  0.3131030105089068\n",
      "step:  5200 , loss_value:  0.25546495110953343\n",
      "step:  5600 , loss_value:  0.21303486955818618\n",
      "step:  6000 , loss_value:  0.1810316779801297\n",
      "step:  6400 , loss_value:  0.15632697269294127\n",
      "step:  6800 , loss_value:  0.1368498077075031\n",
      "step:  7200 , loss_value:  0.12120268123212485\n",
      "step:  7600 , loss_value:  0.10842190832003085\n",
      "step:  8000 , loss_value:  0.09782836685702127\n",
      "step:  8400 , loss_value:  0.08893344976061227\n",
      "step:  8800 , loss_value:  0.08137876901918716\n",
      "step:  9200 , loss_value:  0.07489674086300671\n",
      "step:  9600 , loss_value:  0.06928430661656529\n",
      "step:  10000 , loss_value:  0.06438507131218632\n",
      "Initial loss func:  2.280319056024856\n",
      "step:  0 , loss_value:  2.270695366894201\n",
      "step:  400 , loss_value:  1.8275450417239572\n",
      "step:  800 , loss_value:  1.4599023413459242\n",
      "step:  1200 , loss_value:  1.0979738967559634\n",
      "step:  1600 , loss_value:  0.8065529029933111\n",
      "step:  2000 , loss_value:  0.5937441090921485\n",
      "step:  2400 , loss_value:  0.44621737774710296\n",
      "step:  2800 , loss_value:  0.34513892631585574\n",
      "step:  3200 , loss_value:  0.27490208890148543\n",
      "step:  3600 , loss_value:  0.224835489987755\n",
      "step:  4000 , loss_value:  0.18811943541616097\n",
      "step:  4400 , loss_value:  0.16044827348315638\n",
      "step:  4800 , loss_value:  0.13907029919816247\n",
      "step:  5200 , loss_value:  0.12218783957932561\n",
      "step:  5600 , loss_value:  0.10859668017375654\n",
      "step:  6000 , loss_value:  0.09746965372393736\n",
      "step:  6400 , loss_value:  0.08822490174552801\n",
      "step:  6800 , loss_value:  0.08044401886594223\n",
      "step:  7200 , loss_value:  0.07382002831309321\n",
      "step:  7600 , loss_value:  0.06812355690417178\n",
      "step:  8000 , loss_value:  0.06318035607836033\n",
      "step:  8400 , loss_value:  0.05885605150812317\n",
      "step:  8800 , loss_value:  0.05504559568985045\n",
      "step:  9200 , loss_value:  0.0516658418414913\n",
      "step:  9600 , loss_value:  0.04865022843956156\n",
      "step:  10000 , loss_value:  0.045944916087997785\n",
      "Initial loss func:  2.698398527633598\n",
      "step:  0 , loss_value:  2.6795800466355892\n",
      "step:  400 , loss_value:  2.222076949253035\n",
      "step:  800 , loss_value:  2.1169789122283866\n",
      "step:  1200 , loss_value:  1.9132491377624983\n",
      "step:  1600 , loss_value:  1.7066995996508996\n",
      "step:  2000 , loss_value:  1.5485975118541293\n",
      "step:  2400 , loss_value:  1.424553077991885\n",
      "step:  2800 , loss_value:  1.3214219642594123\n",
      "step:  3200 , loss_value:  1.2283353748157668\n",
      "step:  3600 , loss_value:  1.1308773272339148\n",
      "step:  4000 , loss_value:  1.0081506073389512\n",
      "step:  4400 , loss_value:  0.8512740355810011\n",
      "step:  4800 , loss_value:  0.6834780787176924\n",
      "step:  5200 , loss_value:  0.5349867855302111\n",
      "step:  5600 , loss_value:  0.4185392457929895\n",
      "step:  6000 , loss_value:  0.33214914053130234\n",
      "step:  6400 , loss_value:  0.2689179174157852\n",
      "step:  6800 , loss_value:  0.2222596558024988\n",
      "step:  7200 , loss_value:  0.18722800318180327\n",
      "step:  7600 , loss_value:  0.16038931707992615\n",
      "step:  8000 , loss_value:  0.13941046687080272\n",
      "step:  8400 , loss_value:  0.12270179184308655\n",
      "step:  8800 , loss_value:  0.10916590598698242\n",
      "step:  9200 , loss_value:  0.09803207984874371\n",
      "step:  9600 , loss_value:  0.08874893908991072\n",
      "step:  10000 , loss_value:  0.08091478857199824\n",
      "Initial loss func:  5.4512710801598825\n",
      "step:  0 , loss_value:  5.315278520472956\n",
      "step:  400 , loss_value:  2.7640961348511457\n",
      "step:  800 , loss_value:  2.761472525388338\n",
      "step:  1200 , loss_value:  2.758118195143961\n",
      "step:  1600 , loss_value:  2.7537504840092186\n",
      "step:  2000 , loss_value:  2.747986567603307\n",
      "step:  2400 , loss_value:  2.7403099314665083\n",
      "step:  2800 , loss_value:  2.730028529090216\n",
      "step:  3200 , loss_value:  2.716221104269902\n",
      "step:  3600 , loss_value:  2.6976669577193335\n",
      "step:  4000 , loss_value:  2.67276140275114\n",
      "step:  4400 , loss_value:  2.639446781591164\n",
      "step:  4800 , loss_value:  2.59523623032464\n",
      "step:  5200 , loss_value:  2.5374154425599027\n",
      "step:  5600 , loss_value:  2.4633531801419783\n",
      "step:  6000 , loss_value:  2.370572783508943\n",
      "step:  6400 , loss_value:  2.256313914386227\n",
      "step:  6800 , loss_value:  2.1170095244941827\n",
      "step:  7200 , loss_value:  1.9487335142737225\n",
      "step:  7600 , loss_value:  1.749774989675816\n",
      "step:  8000 , loss_value:  1.5255011643516017\n",
      "step:  8400 , loss_value:  1.2918240045288059\n",
      "step:  8800 , loss_value:  1.070694831461057\n",
      "step:  9200 , loss_value:  0.8791798575428211\n",
      "step:  9600 , loss_value:  0.7232915691593951\n",
      "step:  10000 , loss_value:  0.6004704903272399\n"
     ]
    }
   ],
   "source": [
    "#and\n",
    "xdata=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "tdata=np.array([0,0,0,1])\n",
    "\n",
    "and_obj=LogicGate(\"AND_GATE\",xdata,tdata)\n",
    "and_obj.train()\n",
    "\n",
    "#or\n",
    "xdata=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "tdata=np.array([0,1,1,1])\n",
    "or_obj=LogicGate(\"OR_GATE\",xdata,tdata)\n",
    "or_obj.train()\n",
    "\n",
    "#nand\n",
    "xdata=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "tdata=np.array([1,1,1,0])\n",
    "nand_obj=LogicGate(\"NAND_GATE\",xdata,tdata)\n",
    "nand_obj.train()\n",
    "\n",
    "#xor \n",
    "xdata=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "tdata=np.array([0,1,1,0])\n",
    "xor_obj=LogicGate(\"XOR_GATE\",xdata,tdata)\n",
    "# XOR 게이트\n",
    "xor_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b491bdb5-f181-43a1-abf2-909ae27fdb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND_GATE\n",
      "(array([0.00214761]), 0)\n",
      "[0 0] = 0\n",
      "(array([0.01894615]), 0)\n",
      "[0 1] = 0\n",
      "(array([0.01888828]), 0)\n",
      "[1 0] = 0\n",
      "(array([0.97624794]), 1)\n",
      "[1 1] = 1\n",
      "\n",
      "OR_GATE\n",
      "[0 0] = 0\n",
      "[0 1] = 1\n",
      "[1 0] = 1\n",
      "[1 1] = 1\n",
      "\n",
      "NAND_GATE\n",
      "[0 0] = 1\n",
      "[0 1] = 1\n",
      "[1 0] = 1\n",
      "[1 1] = 0\n",
      "\n",
      "XOR_GATE\n",
      "[0 0] = 0\n",
      "[0 1] = 1\n",
      "[1 0] = 1\n",
      "[1 1] = 0\n"
     ]
    }
   ],
   "source": [
    "# end\n",
    "print(and_obj.name)\n",
    "test_data=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "for input_data in test_data:\n",
    "    print(and_obj.predict(input_data))\n",
    "    (real_val, logical_val)=and_obj.predict(input_data)\n",
    "    print(input_data,\"=\",logical_val)\n",
    "print()\n",
    "# or\n",
    "print(or_obj.name)\n",
    "test_data=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (real_val, logical_val)=or_obj.predict(input_data)\n",
    "    print(input_data,\"=\",logical_val)\n",
    "print()\n",
    "    \n",
    "# nand\n",
    "print(nand_obj.name)\n",
    "test_data=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (real_val, logical_val)=nand_obj.predict(input_data)\n",
    "    print(input_data,\"=\",logical_val)\n",
    "print()\n",
    "\n",
    "# xor 익스큐트오아\n",
    "print(xor_obj.name)\n",
    "test_data=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (real_val, logical_val)=xor_obj.predict(input_data)\n",
    "    print(input_data,\"=\",logical_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70952bb-0569-4a25-88bf-0dd607baeacd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
